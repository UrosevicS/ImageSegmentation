{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884985be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeMask:\n",
    "    def __call__(self, mask):\n",
    "        # Make values be 0, 1 and 2\n",
    "        mask *= 1/0.0039\n",
    "        mask = torch.round(mask)\n",
    "        mask -= 1\n",
    "        return mask\n",
    "    \n",
    "\n",
    "class Flip:\n",
    "    '''\n",
    "    Be careful with tehse calss, it is designed to flip image and mask at same time\n",
    "    but it maybe in case of some interuption would stop doing it at same time,\n",
    "    in other words image will get fliped and mask wont for example.\n",
    "    So just rerun it.\n",
    "    '''\n",
    "    def __init__(self, seed=42):\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        if self.rng.rand(1) > 0.5:\n",
    "            img = transforms.functional.hflip(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "# These is already normalized form 0 to 1\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.ToTensor(),\n",
    "    Flip(),\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128), interpolation= Image.NEAREST),  \n",
    "    transforms.ToTensor(),\n",
    "    NormalizeMask(),\n",
    "    Flip(),\n",
    "    transforms.Lambda(lambda x: x.long())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Oxford-IIIT Pet dataset\n",
    "train_dataset = datasets.OxfordIIITPet(root='./data',target_types=\"segmentation\", \n",
    "                                 transform=img_transform,\n",
    "                                 target_transform=mask_transform, \n",
    "                                 split=\"trainval\",\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.OxfordIIITPet(root='./data',target_types=\"segmentation\", \n",
    "                                 transform=img_transform,\n",
    "                                 target_transform=mask_transform, \n",
    "                                 split=\"test\",\n",
    "                                 download=True)\n",
    "# Create a DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in train_loader:\n",
    "    sample_img, sample_mask = images[0].permute(1,2,0), masks[0].permute(1,2,0)\n",
    "    display([sample_img, sample_mask])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a501a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2):\n",
    "        super(UpBlock, self).__init__()\n",
    "        padding = 1\n",
    "        output_padding = 1\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                                         kernel_size=kernel_size, stride=stride, \n",
    "                                         padding=padding, output_padding=output_padding, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Apply the random normal initializer to the ConvTranspose2d weights\n",
    "        if hasattr(self.upconv, 'weight') and self.upconv.weight is not None:\n",
    "            init.normal_(self.upconv.weight, mean=0., std=0.02)\n",
    "\n",
    "        # Optionally, initialize biases (if your layer has biases)\n",
    "        if hasattr(self.upconv, 'bias') and self.upconv.bias is not None:\n",
    "            init.constant_(self.upconv.bias, 0.)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.upconv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "up_stack = nn.Sequential(\n",
    "    UpBlock(320, 512),\n",
    "    UpBlock(1088, 256),\n",
    "    UpBlock(448, 128),\n",
    "    UpBlock(272, 64),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b402af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowPicCallback(Callback):\n",
    "    def __init__(self, img, mask):\n",
    "        super().__init__()\n",
    "        #img and mask shape = C, H, W\n",
    "        self.img = img.unsqueeze(0)\n",
    "        self.mask = mask.permute(1,2,0)\n",
    "        \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        prediction = pl_module(self.img)\n",
    "        prediction = pl_module.create_mask(prediction, 'whc')[0]\n",
    "        \n",
    "        display([self.img[0].permute(1,2,0),\n",
    "                self.mask,\n",
    "                prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adc6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "# Layers we wont access to to make skip connections\n",
    "down_stack = [\n",
    "    mobilenet_v2.features[2].conv[0],\n",
    "    mobilenet_v2.features[4].conv[0],\n",
    "    mobilenet_v2.features[7].conv[0],\n",
    "    mobilenet_v2.features[14].conv[0],\n",
    "    mobilenet_v2.features[17].conv[2],\n",
    "]\n",
    "# Just remove these layers\n",
    "mobilenet_v2.classifier = IdentityLayer()\n",
    "mobilenet_v2.features[18] = IdentityLayer()\n",
    "\n",
    "for param in mobilenet_v2.parameters():\n",
    "    param.requires_grad = False\n",
    "        \n",
    "class SeqmentationModel(L.LightningModule):\n",
    "    def __init__(self, mob_net, down_stack, up_stack, \n",
    "                 padding=1, output_padding=1,\n",
    "                 loss=nn.CrossEntropyLoss()):\n",
    "        super().__init__()\n",
    "        self.mob_net = mob_net\n",
    "        self.down_stack = down_stack\n",
    "        self.up_stack = up_stack\n",
    "        \n",
    "        self.last = nn.ConvTranspose2d(160, 3, kernel_size=3, stride=2, padding=padding, output_padding=output_padding)\n",
    "        \n",
    "        self.criterion = loss\n",
    "        self.accuracy_metric = Accuracy(num_classes=3, task=\"multiclass\")\n",
    "        self.history = defaultdict(lambda:[])\n",
    "        \n",
    "        self.add_hooks()\n",
    "        \n",
    "    def add_hooks(self):\n",
    "        self.skips = OrderedDict()\n",
    "        self.hooks = []\n",
    "        def f(m,i,o,ind):\n",
    "            self.skips[str(m)]=o\n",
    "        for ind, layer in enumerate(self.down_stack):\n",
    "            self.hooks.append(layer.register_forward_hook(lambda m,i,o:f(m,i,o,ind)))\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        self.hooks = []\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "            \n",
    "    def forward(self, images):\n",
    "        x = images\n",
    "        # down sample image\n",
    "        self.mob_net(x)\n",
    "\n",
    "        skips = list(self.skips.values())\n",
    "        x = skips.pop()\n",
    "        skips = list(reversed(skips))\n",
    "        \n",
    "        # up sample image\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        \n",
    "        x = self.last(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        accuracy, loss, predictions = self._common_step(batch, batch_idx)\n",
    "        \n",
    "        self.training_step_outputs = {\n",
    "            'training_loss':loss,\n",
    "            'training_accuracy':accuracy\n",
    "        }\n",
    "        self.log_dict(self.training_step_outputs, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        accuracy, loss, predictions = self._common_step(batch, batch_idx)\n",
    "        \n",
    "        self.validation_step_outputs = {\n",
    "            'validation_loss':loss,\n",
    "            'validation_accuracy':accuracy\n",
    "        }\n",
    "        self.log_dict(self.validation_step_outputs, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        accuracy, loss, predictions = self._common_step(batch, batch_idx)\n",
    "        \n",
    "        self.test_step_outputs = {\n",
    "            'test_loss':loss,\n",
    "            'test_accuracy':accuracy\n",
    "        }\n",
    "        self.log_dict(self.test_step_outputs, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        predictions = self.forward(images)\n",
    "        loss = self.criterion(predictions, masks.squeeze(1))\n",
    "        accuracy = self.accuracy_metric(self.create_mask(predictions), masks)\n",
    "        return accuracy, loss, predictions\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        self._common_on_batch_end(self.training_step_outputs)\n",
    "    \n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx):\n",
    "        self._common_on_batch_end(self.validation_step_outputs)\n",
    "    \n",
    "    def on_test_batch_end(self, outputs, batch, batch_idx):\n",
    "        self._common_on_batch_end(self.test_step_outputs)\n",
    "    \n",
    "    def _common_on_batch_end(self, step_outputs):\n",
    "        loss, accuracy = step_outputs.keys()\n",
    "        self.history[loss].append(step_outputs[loss])\n",
    "        self.history[accuracy].append(step_outputs[accuracy])\n",
    "    \n",
    "    def create_mask(self, pred_mask, shape='cwh'):\n",
    "        pred_mask = torch.argmax(pred_mask, axis=1)\n",
    "        if shape == 'cwh':\n",
    "            pred_mask = pred_mask.unsqueeze(1)\n",
    "        else:\n",
    "            pred_mask = pred_mask.unsqueeze(-1)\n",
    "        return pred_mask\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "model = SeqmentationModel(mobilenet_v2, down_stack, up_stack)\n",
    "trainer = L.Trainer(\n",
    "    min_epochs=10, \n",
    "    max_epochs=20,\n",
    "    callbacks=[ShowPicCallback(images[0], masks[0])])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6177fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.create_mask(model(images), 'whc')[0]\n",
    "display([sample_img, sample_mask,predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(model.history['training_loss']).numpy(), label='training_loss')\n",
    "plt.plot(torch.tensor(model.history['training_accuracy']).numpy(), label='training_accuracy')\n",
    "plt.plot(torch.tensor(model.history['validation_loss']).numpy(), label='validation_loss')\n",
    "plt.plot(torch.tensor(model.history['validation_accuracy']).numpy(), label='validation_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e45791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_torch.pth'\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
